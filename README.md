# ğŸ¦™ Ultraâ€‘Efficient Fineâ€‘Tuning of **LlamaÂ 3.1Â 8B** with **Unsloth**

*(A handsâ€‘on guide to turning an offâ€‘theâ€‘shelf LLM into **your** bespoke assistant)*

> **TL;DR** â€“ This repo walks through a GoogleÂ Colab workflow that fineâ€‘tunes the `Metaâ€‘Llamaâ€‘3.1â€‘8B` model with **QLoRA** adapters using the blazingâ€‘fast **Unsloth** backend. Youâ€™ll learn **why** (and **when**) to fineâ€‘tune, how to pick the right hyperâ€‘parameters, and how to ship your model in both HuggingÂ Face and GGUF formats â€“ all on a single GPU.

---

## ğŸš€ Why Fineâ€‘Tune Instead of Just Prompting?

1. **Domain mastery** â€“ Teach the model proprietary jargon or workflows it has never seen.
2. **Lower latency & cost** â€“ Swap noisy RAG calls for a compact model that â€œjust knowsâ€.
3. **Style control** â€“ Bake in tone, brand voice, or compliance constraints.
4. **Offline / edge** â€“ Run slimmer quants locally without recurring API calls.

*If promptâ€‘engineering or RAG already nails your useâ€‘case, stick with them. Otherwise, supervised fineâ€‘tuning (SFT) is the next logical step.*

---

## ğŸ”§ The Three Roads to SFT

| Technique          | TrainableÂ Params   | VRAM Needs | Speed | Pros                                        | Cons                                                |
| ------------------ | ------------------ | ---------- | ----- | ------------------------------------------- | --------------------------------------------------- |
| **Full fineâ€‘tune** | 100Â %              | ğŸ’ğŸ’ğŸ’     | ğŸ¢    | Best raw quality                            | Requires multiâ€‘GPU, risk of catastrophic forgetting |
| **LoRA**           | <â€¯1â€¯%              | ğŸ’         | ğŸš€    | Parameterâ€‘efficient, plugâ€‘andâ€‘play adapters | Slight quality drop if rank is too low              |
| **QLoRA**          | <â€¯1â€¯% (4â€‘bit base) | ğŸª™         | ğŸš€ğŸš€  | Even lower memory; train on consumer GPUs   | \~39â€¯% slower than LoRA                             |

*Weâ€™ll use **QLoRA** because it fits an 8â€¯B model into \~5.4â€¯GB, perfect for free Colab environments.*

---

## ğŸ› ï¸ Project Structure

```
ğŸ“‚ fineâ€‘tuneâ€‘llamaâ€‘3.1
 â”œâ”€â”€ notebook.ipynb  # endâ€‘toâ€‘end Colab script
 â”œâ”€â”€ data/           # instruction dataset (ShareGPT JSONL)
 â”œâ”€â”€ output/         # trainer checkpoints & logs
 â””â”€â”€ README.md       # youâ€™re here
```

---

## âš¡ QuickÂ Start

```bash
# 1. Setup (Colab or local GPU)
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

```python
# 2. Load 4â€‘bit base model
from unsloth import FastLanguageModel, is_bfloat16_supported
max_seq_len = 2048
model, tok = FastLanguageModel.from_pretrained(
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
    max_seq_length=max_seq_len,
    load_in_4bit=True,
    dtype=None,
)
```

```python
# 3. Attach Rankâ€‘Stabilised LoRA adapters (r = 16)
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    lora_alpha=16,
    lora_dropout=0.0,
    target_modules=["q_proj","k_proj","v_proj","up_proj","down_proj","o_proj","gate_proj"],
    use_rslora=True,
    use_gradient_checkpointing="unsloth",
)
```

```python
# 4. Convert ShareGPT â†’ ChatML and load dataset
from datasets import load_dataset
from unsloth.chat_templates import get_chat_template

tok = get_chat_template(tok,
    mapping={"role":"from","content":"value","user":"human","assistant":"gpt"},
    chat_template="chatml")

def to_chatml(batch):
    txt = [tok.apply_chat_template(m, tokenize=False) for m in batch["conversations"]]
    return {"text": txt}

data = load_dataset("<your-dataset>", split="train[:10%]")  # slice for quick tests
data = data.map(to_chatml, batched=True)
```

```python
# 5. Train
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tok,
    train_dataset=data,
    dataset_text_field="text",
    max_seq_length=max_seq_len,
    packing=True,
    args=TrainingArguments(
        output_dir="output",
        per_device_train_batch_size=8,
        gradient_accumulation_steps=2,
        num_train_epochs=1,
        learning_rate=3e-4,
        lr_scheduler_type="linear",
        warmup_steps=10,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        optim="adamw_8bit",
        weight_decay=0.01,
        logging_steps=1,
        seed=42,
    ),
)
trainer.train()
```

```python
# 6. Sanityâ€‘check
model = FastLanguageModel.for_inference(model)
msg = [{"from":"human","value":"Is 9.11 larger than 9.9?"}]
ids = tok.apply_chat_template(msg, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
print(tok.decode(model.generate(ids, max_new_tokens=16)[0]))
```

```python
# 7. Save & merge adapters (16â€‘bit) then push to Hub
model.save_pretrained_merged("model", tok, save_method="merged_16bit")
model.push_to_hub_merged("<yourâ€‘hfâ€‘org>/FineLlamaâ€‘3.1â€‘8B", tok, save_method="merged_16bit")
```

```python
# 8. Optional: oneâ€‘liner GGUF export for local inference
for q in ["q4_k_m", "q5_k_m", "q8_0"]:
    model.push_to_hub_gguf("<yourâ€‘hfâ€‘org>/FineLlamaâ€‘3.1â€‘8Bâ€‘GGUF", tok, q)
```

---

## ğŸ§© Understanding the Knobs

### LoRAâ€‘Specific

| Hyperâ€‘param          | Typical Range         | Effect                                              |
| -------------------- | --------------------- | --------------------------------------------------- |
| **`r` (rank)**       | 8â€‘64 (this guide:Â 16) | LargerÂ = captures more task info but uses more VRAM |
| **`lora_alpha`**     | 1Ã—â€“2Ã—Â `r`             | Scaling factor for adapter updates                  |
| **`target_modules`** | q/k/v, proj layers    | More targetsÂ = higher quality, higher cost          |
| **`use_rslora`**     | True/False            | Stabilises training for large `r` via 1/âˆšr scaling  |

### Trainer Hyperâ€‘params

| Name                      | Why it matters                                                        |
| ------------------------- | --------------------------------------------------------------------- |
| `learning_rate`           | Too high â†’ divergence; too low â†’ sluggish training                    |
| `lr_scheduler_type`       | Linear is safe; cosine sometimes yields better lateâ€‘stage convergence |
| `batch_size Ã— grad_accum` | Defines *effective* batch; crank up until you hit VRAM ceiling        |
| `num_train_epochs`        | Stop at the first signs of overâ€‘fitting (eval loss plateau)           |
| `weight_decay`            | 0.01 keeps weights in check; tweak if you notice overâ€‘regularization  |
| `warmup_steps`            | \~0.1â€‘1Â % of total steps smooths the learningâ€‘rate ramp               |

---

## ğŸ“Š Evaluating Your New Model

1. **Automated** â€“ Submit to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) or run `lmâ€‘evalâ€‘harness` locally.
2. **Human in the loop** â€“ Spin up an inference UI (Ollama, LMÂ Studio) and sanityâ€‘check real prompts.
3. **Taskâ€‘specific** â€“ Build regression/classification benchmarks using your own goldâ€‘label data.

---

## ğŸ—ï¸ NextÂ Steps

* **Alignment** â€“ Run **DPO** / ORPO on preference data to shape style & safety.
* **Quantization** â€“ Try EXL2, GPTQ, AWQ for even lighter deployments.
* **Deployment** â€“ Serve via a HuggingÂ Face Space, Nvidia Triton, or even onâ€‘device with llama.cpp.

---

## ğŸ“œ License & Credits

Everything in this repo is released under the ApacheÂ 2.0 license. Feel free to fork, extend, and â€“ most importantly â€“ *ship great products*.

> *Replace this block with your personal bio, social links, and a heartfelt shoutâ€‘out to the openâ€‘source community that made all of this possible.*

---

### Happy fineâ€‘tuning! ğŸ§‘â€ğŸ”§ğŸ‰
